{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "main_path = Path('..').resolve()\n",
    "sys.path.append(str(main_path))\n",
    "\n",
    "from src.dataset import MetaStockDataset\n",
    "from src.utils import ARGProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data and candidates for train: 100%|██████████| 40/40 [00:00<00:00, 40.62it/s]\n"
     ]
    }
   ],
   "source": [
    "setting_file = Path('.') / 'kdd.yml'\n",
    "\n",
    "meta_args = ARGProcessor(setting_file=setting_file)\n",
    "data_kwargs = meta_args.get_args(cls=MetaStockDataset)\n",
    "\n",
    "meta_train = MetaStockDataset(meta_type='train', **data_kwargs)\n",
    "# meta_test1 = MetaStockDataset(meta_type='test1', **data_kwargs)\n",
    "# meta_test2 = MetaStockDataset(meta_type='test2', **data_kwargs)\n",
    "# meta_test3 = MetaStockDataset(meta_type='test3', **data_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = meta_train.generate_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockDataDict(T=5, numpy)\n",
      "- query: (40, 5, 2, 5, 11)\n",
      "- query_labels: (40, 5)\n",
      "- support: (40, 5, 4, 5, 11)\n",
      "- support_labels: (40, 5)\n",
      "StockDataDict(T=10, numpy)\n",
      "- query: (40, 5, 2, 10, 11)\n",
      "- query_labels: (40, 5)\n",
      "- support: (40, 5, 4, 10, 11)\n",
      "- support_labels: (40, 5)\n",
      "StockDataDict(T=15, numpy)\n",
      "- query: (40, 5, 2, 15, 11)\n",
      "- query_labels: (40, 5)\n",
      "- support: (40, 5, 4, 15, 11)\n",
      "- support_labels: (40, 5)\n",
      "StockDataDict(T=20, numpy)\n",
      "- query: (40, 5, 2, 20, 11)\n",
      "- query_labels: (40, 5)\n",
      "- support: (40, 5, 4, 20, 11)\n",
      "- support_labels: (40, 5)\n"
     ]
    }
   ],
   "source": [
    "sym = meta_train.symbols[0]\n",
    "\n",
    "for win_size, d in all_data.items():\n",
    "    print(f'{d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockDataDict(T=5, tensor.cpu)\n",
       "- query: torch.Size([40, 5, 2, 5, 11])\n",
       "- query_labels: torch.Size([40, 5])\n",
       "- support: torch.Size([40, 5, 4, 5, 11])\n",
       "- support_labels: torch.Size([40, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data = all_data[5]\n",
    "stock_data.to('cpu')\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(stock_data):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for all stocks\n",
    "q_inputs = data['query']\n",
    "q_labels = data['query_labels']\n",
    "s_inputs = data['support']\n",
    "s_labels = data['support_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "symbol = 'NVS'\n",
    "df_stock = meta_train.data[symbol]\n",
    "# filter out unpossible candidates\n",
    "labels_indices = meta_train.candidates[symbol] \n",
    "labels_indices = labels_indices[labels_indices >= window_size]\n",
    "\n",
    "for i in range(len(labels_indices)):\n",
    "    array = df_stock.loc[labels_indices, 'label'].loc[:(labels_indices[i])].to_numpy()\n",
    "    \n",
    "    if meta_train.check_condition(array):\n",
    "        break\n",
    "\n",
    "# satisfied condition label index | smallest support index | smallest query index\n",
    "candidates = labels_indices[(i+2):]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   6,    7,    8, ..., 1980, 1983, 1984], dtype=int64),\n",
       " array([  16,   18,   19, ..., 1980, 1983, 1984], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_indices, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>zd5</th>\n",
       "      <th>zd10</th>\n",
       "      <th>zd15</th>\n",
       "      <th>zd20</th>\n",
       "      <th>zd25</th>\n",
       "      <th>zd30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2007-02-23</td>\n",
       "      <td>-0.426257</td>\n",
       "      <td>0.289852</td>\n",
       "      <td>-0.750218</td>\n",
       "      <td>1.312840</td>\n",
       "      <td>1.312837</td>\n",
       "      <td>-0.208016</td>\n",
       "      <td>0.134694</td>\n",
       "      <td>0.155723</td>\n",
       "      <td>-0.254052</td>\n",
       "      <td>-0.374427</td>\n",
       "      <td>-0.344987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007-02-26</td>\n",
       "      <td>0.647764</td>\n",
       "      <td>1.523115</td>\n",
       "      <td>-0.367645</td>\n",
       "      <td>-2.608701</td>\n",
       "      <td>-2.608699</td>\n",
       "      <td>1.722689</td>\n",
       "      <td>2.594538</td>\n",
       "      <td>2.670401</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>2.193978</td>\n",
       "      <td>2.272993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007-02-27</td>\n",
       "      <td>2.006508</td>\n",
       "      <td>2.042663</td>\n",
       "      <td>-0.216918</td>\n",
       "      <td>-3.151259</td>\n",
       "      <td>-3.151259</td>\n",
       "      <td>3.814171</td>\n",
       "      <td>5.379608</td>\n",
       "      <td>5.642322</td>\n",
       "      <td>5.501626</td>\n",
       "      <td>5.370209</td>\n",
       "      <td>5.427813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>-0.725947</td>\n",
       "      <td>0.272236</td>\n",
       "      <td>-1.306710</td>\n",
       "      <td>-0.595349</td>\n",
       "      <td>-0.595349</td>\n",
       "      <td>2.221419</td>\n",
       "      <td>4.411981</td>\n",
       "      <td>5.133699</td>\n",
       "      <td>5.497281</td>\n",
       "      <td>5.353905</td>\n",
       "      <td>5.370844</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007-03-05</td>\n",
       "      <td>-0.073222</td>\n",
       "      <td>0.823725</td>\n",
       "      <td>-0.237966</td>\n",
       "      <td>-0.509924</td>\n",
       "      <td>-0.509924</td>\n",
       "      <td>0.820061</td>\n",
       "      <td>3.589601</td>\n",
       "      <td>5.120506</td>\n",
       "      <td>5.717553</td>\n",
       "      <td>5.801939</td>\n",
       "      <td>5.846603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2007-03-06</td>\n",
       "      <td>-1.143354</td>\n",
       "      <td>0.386987</td>\n",
       "      <td>-1.495159</td>\n",
       "      <td>4.063696</td>\n",
       "      <td>6.164447</td>\n",
       "      <td>-4.110703</td>\n",
       "      <td>-2.544715</td>\n",
       "      <td>-1.027276</td>\n",
       "      <td>-0.453909</td>\n",
       "      <td>-0.317484</td>\n",
       "      <td>-0.263891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2007-03-07</td>\n",
       "      <td>-0.424630</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>-0.477707</td>\n",
       "      <td>-0.580471</td>\n",
       "      <td>-0.580474</td>\n",
       "      <td>-2.777016</td>\n",
       "      <td>-2.063988</td>\n",
       "      <td>-0.566073</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.273612</td>\n",
       "      <td>0.287271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2007-03-09</td>\n",
       "      <td>-0.195486</td>\n",
       "      <td>0.373201</td>\n",
       "      <td>-0.284342</td>\n",
       "      <td>-0.758379</td>\n",
       "      <td>-0.758378</td>\n",
       "      <td>-0.519284</td>\n",
       "      <td>-1.853495</td>\n",
       "      <td>-0.584123</td>\n",
       "      <td>0.225632</td>\n",
       "      <td>0.654348</td>\n",
       "      <td>0.653893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2007-03-13</td>\n",
       "      <td>0.623891</td>\n",
       "      <td>1.408201</td>\n",
       "      <td>-0.089125</td>\n",
       "      <td>-0.707968</td>\n",
       "      <td>-0.707966</td>\n",
       "      <td>0.566848</td>\n",
       "      <td>-1.130956</td>\n",
       "      <td>-0.638941</td>\n",
       "      <td>0.363629</td>\n",
       "      <td>0.814907</td>\n",
       "      <td>0.940451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2007-03-14</td>\n",
       "      <td>-0.106196</td>\n",
       "      <td>0.159292</td>\n",
       "      <td>-0.707968</td>\n",
       "      <td>0.713016</td>\n",
       "      <td>0.713014</td>\n",
       "      <td>-0.152211</td>\n",
       "      <td>-1.447406</td>\n",
       "      <td>-1.403617</td>\n",
       "      <td>-0.436208</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.232221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close  adj_close       zd5  \\\n",
       "6  2007-02-23 -0.426257  0.289852 -0.750218  1.312840   1.312837 -0.208016   \n",
       "7  2007-02-26  0.647764  1.523115 -0.367645 -2.608701  -2.608699  1.722689   \n",
       "8  2007-02-27  2.006508  2.042663 -0.216918 -3.151259  -3.151259  3.814171   \n",
       "10 2007-03-01 -0.725947  0.272236 -1.306710 -0.595349  -0.595349  2.221419   \n",
       "12 2007-03-05 -0.073222  0.823725 -0.237966 -0.509924  -0.509924  0.820061   \n",
       "13 2007-03-06 -1.143354  0.386987 -1.495159  4.063696   6.164447 -4.110703   \n",
       "14 2007-03-07 -0.424630  1.008493 -0.477707 -0.580471  -0.580474 -2.777016   \n",
       "16 2007-03-09 -0.195486  0.373201 -0.284342 -0.758379  -0.758378 -0.519284   \n",
       "18 2007-03-13  0.623891  1.408201 -0.089125 -0.707968  -0.707966  0.566848   \n",
       "19 2007-03-14 -0.106196  0.159292 -0.707968  0.713016   0.713014 -0.152211   \n",
       "\n",
       "        zd10      zd15      zd20      zd25      zd30  label  \n",
       "6   0.134694  0.155723 -0.254052 -0.374427 -0.344987      1  \n",
       "7   2.594538  2.670401  2.380952  2.193978  2.272993      0  \n",
       "8   5.379608  5.642322  5.501626  5.370209  5.427813      0  \n",
       "10  4.411981  5.133699  5.497281  5.353905  5.370844      0  \n",
       "12  3.589601  5.120506  5.717553  5.801939  5.846603      0  \n",
       "13 -2.544715 -1.027276 -0.453909 -0.317484 -0.263891      1  \n",
       "14 -2.063988 -0.566073  0.000786  0.273612  0.287271      0  \n",
       "16 -1.853495 -0.584123  0.225632  0.654348  0.653893      0  \n",
       "18 -1.130956 -0.638941  0.363629  0.814907  0.940451      0  \n",
       "19 -1.447406 -1.403617 -0.436208 -0.001494  0.232221      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.loc[labels_indices].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "query index: 1149(1984) = 0\n",
      "query start=[1978 1975] end=[1983 1980]\n",
      "support index: 1148(1983) = 0\n",
      "support start=[1974 1968 1975 1973] end=[1979 1973 1980 1978]\n",
      "\n",
      "query index: 1036(1748) = 0\n",
      "query start=[1741 1742] end=[1746 1747]\n",
      "support index: 1035(1747) = 1\n",
      "support start=[1741 1739 1738 1734] end=[1746 1744 1743 1739]\n",
      "\n",
      "query index: 707(1126) = 1\n",
      "query start=[1120 1114] end=[1125 1119]\n",
      "support index: 706(1125) = 0\n",
      "support start=[1117 1116 1114 1110] end=[1122 1121 1119 1115]\n",
      "\n",
      "query index: 785(1230) = 1\n",
      "query start=[1223 1221] end=[1228 1226]\n",
      "support index: 784(1228) = 0\n",
      "support start=[1216 1212 1221 1217] end=[1221 1217 1226 1222]\n",
      "\n",
      "query index: 919(1516) = 1\n",
      "query start=[1510 1508] end=[1515 1513]\n",
      "support index: 918(1515) = 0\n",
      "support start=[1501 1496 1508 1507] end=[1506 1501 1513 1512]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(999)\n",
    "\n",
    "data = dict(\n",
    "    query = [],\n",
    "    query_labels = [],\n",
    "    query_masks = [],\n",
    "    support = [],\n",
    "    support_labels = [],\n",
    "    support_masks = []\n",
    ")\n",
    "\n",
    "y_s = np.random.choice(candidates, size=(meta_train.n_sample,), replace=False)   # index in the dataframe\n",
    "y_q = np.concatenate([[candidates[-1]], y_s[:-1]])\n",
    "for q_target in y_q:\n",
    "    # Queries\n",
    "    q_idx = np.arange(len(labels_indices))[labels_indices == q_target][0]  # get the index of label data\n",
    "    q_fall, q_rise = meta_train.get_rise_fall(df_stock, labels_indices, idx=q_idx, n_select=meta_train.n_query)\n",
    "    q_end = np.concatenate([q_fall, q_rise])\n",
    "    q_start = q_end - window_size\n",
    "    q_data, q_mask = meta_train.generate_data(df_stock, y_start=q_start, y_end=q_end)\n",
    "\n",
    "    data['query'].append(q_data)\n",
    "    data['query_masks'].append(q_mask)\n",
    "    data['query_labels'].append(df_stock.loc[q_target, 'label'])\n",
    "\n",
    "    # Supports\n",
    "    s_idx = q_idx - 1\n",
    "    s_target = labels_indices[s_idx]\n",
    "    s_fall, s_rise = meta_train.get_rise_fall(df_stock, labels_indices, idx=s_idx, n_select=meta_train.n_support)\n",
    "    s_end = np.concatenate([s_fall, s_rise])\n",
    "    s_start = s_end - window_size\n",
    "    s_data, s_mask = meta_train.generate_data(df_stock, y_start=s_start, y_end=s_end)\n",
    "    \n",
    "    data['support'].append(s_data)\n",
    "    data['support_masks'].append(s_mask)\n",
    "    data['support_labels'].append(df_stock.loc[s_target, 'label'])\n",
    "\n",
    "    print()   \n",
    "    print(f'query index: {q_idx}({q_target}) = {df_stock.loc[q_target, \"label\"]}')\n",
    "    print(f'query start={q_start} end={q_end}')\n",
    "    print(f'support index: {s_idx}({s_target}) = {df_stock.loc[s_target, \"label\"]}')\n",
    "    print(f'support start={s_start} end={s_end}')\n",
    "\n",
    "for k, v in data.items():\n",
    "    data[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.model import MetaModel\n",
    "\n",
    "model_kwargs = meta_args.get_args(cls=MetaModel)\n",
    "model = MetaModel(**model_kwargs)\n",
    "\n",
    "rt_attn = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`l` Outputs: torch.Size([5, 4, 5]), torch.Size([5, 4, 5])\n",
      "tensor([[ 1.5363,  0.4500, -0.5009, -1.4776, -0.0078],\n",
      "        [ 1.6298,  0.3652, -0.6980, -1.3117,  0.0147],\n",
      "        [ 1.5484,  0.4783, -0.3586, -1.4862, -0.1819],\n",
      "        [ 1.4959,  0.3614, -0.7076, -1.4315,  0.2818]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encode_lstm\n",
    "l, attn = model.encode_lstm(s_inputs, rt_attn=rt_attn)  # lstm_encoded: (B, N*K, E)\n",
    "print(f'`l` Outputs: {l.size()}, {attn.size()}')\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if isinstance(attn, torch.Tensor):\n",
    "    attn_numpy = attn.detach().numpy()\n",
    "else:\n",
    "    attn_numpy = attn\n",
    "masks = [0, 0, 1, 1]\n",
    "\n",
    "B = attn_numpy.shape[0]\n",
    "fig, axes = plt.subplots(1, B, figsize=(12, 10))\n",
    "for i in range(B):\n",
    "    ax = axes[i]\n",
    "    ax.matshow(attn_numpy[i])\n",
    "    ax.set_title(f'Data {i}')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yticks(np.arange(len(masks)))\n",
    "    ax.set_yticklabels(masks)\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_xlabel('Time Stamp')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`encoded` Outputs: torch.Size([5, 2, 2, 3])\n",
      "tensor([[[-0.2768,  0.0151, -0.5293],\n",
      "         [-0.2455, -0.0271, -0.5426]],\n",
      "\n",
      "        [[-0.3852, -0.0499, -0.5868],\n",
      "         [-0.0928,  0.1028, -0.4336]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encode_linear\n",
    "# Reshape the size\n",
    "B = l.size(0)\n",
    "N = model.output_size\n",
    "K = l.size(1) // N\n",
    "if rt_attn:\n",
    "    attn = attn.view(B, N, K, -1)  # attn: (B, N, K, T)\n",
    "l_reshape = l.view(B, N, K, -1)  # l_reshape: (B, N, K, E)\n",
    "e = model.encoder(l_reshape)  # e: (B, N, K, H)\n",
    "print(f'`encoded` Outputs: {e.size()}')\n",
    "print(e[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Net: class-conditional multivariate Gaussian distribution with a diagonal covariance\n",
    "\n",
    "The paper concatenate tensors for relation net inputs.\n",
    "\n",
    "Let $R(x_{i}^{p}, x_{j}^{q})$ to represent the inputs of hidden state on concatenated relations between classes, $i, j$ for shot index, $p, q$ for class index.\n",
    "\n",
    "The tensor shape is $(B, N^2, K^2, 2H)$. For each data(row) in $B$, the data relationship is $\\sum_{i, j}^N \\sum_{p, q}^{K} R(x_{i}^{p}, x_{j}^{q})$\n",
    "\n",
    "e.g.,  N way K shot = 2 way 2 shot\n",
    "\n",
    "| Relation | Left | Right |\n",
    "|---|---|---|\n",
    "| $R(x_0^0, x_0^0)$ | $h_{K_0}^{N_0}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_0^0, x_1^0)$ | $h_{K_0}^{N_0}$ | $h_{K_1}^{N_0}$ | \n",
    "| $R(x_1^0, x_1^0)$ | $h_{K_1}^{N_0}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_1^0, x_0^0)$ | $h_{K_1}^{N_0}$ | $h_{K_1}^{N_0}$ | \n",
    "| | | |\n",
    "| $R(x_0^0, x_0^1)$ | $h_{K_0}^{N_0}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_0^0, x_1^1)$ | $h_{K_0}^{N_0}$ | $h_{K_1}^{N_1}$ | \n",
    "| $R(x_1^0, x_1^1)$ | $h_{K_1}^{N_0}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_1^0, x_0^1)$ | $h_{K_1}^{N_0}$ | $h_{K_1}^{N_1}$ | \n",
    "| | | |\n",
    "| $R(x_0^1, x_0^0)$ | $h_{K_0}^{N_1}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_0^1, x_1^0)$ | $h_{K_0}^{N_1}$ | $h_{K_1}^{N_0}$ | \n",
    "| $R(x_1^1, x_1^0)$ | $h_{K_1}^{N_1}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_1^1, x_0^0)$ | $h_{K_1}^{N_1}$ | $h_{K_1}^{N_0}$ | \n",
    "| | | |\n",
    "| $R(x_0^1, x_0^1)$ | $h_{K_0}^{N_1}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_0^1, x_1^1)$ | $h_{K_0}^{N_1}$ | $h_{K_1}^{N_1}$ | \n",
    "| $R(x_1^1, x_1^1)$ | $h_{K_1}^{N_1}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_1^1, x_0^1)$ | $h_{K_1}^{N_1}$ | $h_{K_1}^{N_1}$ | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6385, -0.1383, -0.6338,  0.6385, -0.1383, -0.6338],\n",
       "          [ 0.6385, -0.1383, -0.6338,  1.0864, -0.5861, -0.2296],\n",
       "          [ 1.0864, -0.5861, -0.2296,  0.6385, -0.1383, -0.6338],\n",
       "          [ 1.0864, -0.5861, -0.2296,  1.0864, -0.5861, -0.2296]],\n",
       "\n",
       "         [[ 0.6385, -0.1383, -0.6338, -1.5160,  0.4863,  0.1744],\n",
       "          [ 0.6385, -0.1383, -0.6338,  0.1482,  0.0818,  1.0803],\n",
       "          [ 1.0864, -0.5861, -0.2296, -1.5160,  0.4863,  0.1744],\n",
       "          [ 1.0864, -0.5861, -0.2296,  0.1482,  0.0818,  1.0803]],\n",
       "\n",
       "         [[-1.5160,  0.4863,  0.1744,  0.6385, -0.1383, -0.6338],\n",
       "          [-1.5160,  0.4863,  0.1744,  1.0864, -0.5861, -0.2296],\n",
       "          [ 0.1482,  0.0818,  1.0803,  0.6385, -0.1383, -0.6338],\n",
       "          [ 0.1482,  0.0818,  1.0803,  1.0864, -0.5861, -0.2296]],\n",
       "\n",
       "         [[-1.5160,  0.4863,  0.1744, -1.5160,  0.4863,  0.1744],\n",
       "          [-1.5160,  0.4863,  0.1744,  0.1482,  0.0818,  1.0803],\n",
       "          [ 0.1482,  0.0818,  1.0803, -1.5160,  0.4863,  0.1744],\n",
       "          [ 0.1482,  0.0818,  1.0803,  0.1482,  0.0818,  1.0803]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.\n",
    "a = torch.randn(1, 2, 2, 3)\n",
    "left = torch.repeat_interleave(a, 2, dim=2)\n",
    "left = torch.repeat_interleave(left, 2, dim=1)\n",
    "right = a.repeat((1, 2, 2, 1))\n",
    "temp = torch.cat([left, right], dim=-1)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after relation network, average the values for each class for all shots($K$)\n",
    "\n",
    "e.g.,  N way K shot = 2 way 2 shot\n",
    "\n",
    "| Class | Relation |\n",
    "|---|---|\n",
    "| 0 | $f\\big( R(x_0^0, x_0^0) \\big)$ |\n",
    "| 0 | $f\\big( R(x_0^0, x_1^0) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_1^0) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_0^0) \\big)$ | \n",
    "| 0 | $f\\big( R(x_0^0, x_0^1) \\big)$ |\n",
    "| 0 | $f\\big( R(x_0^0, x_1^1) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_1^1) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_0^1) \\big)$ |\n",
    "|   | |\n",
    "| 1 | $f\\big( R(x_0^1, x_0^0) \\big)$ | \n",
    "| 1 | $f\\big( R(x_0^1, x_1^0) \\big)$ | \n",
    "| 1 | $f\\big( R(x_1^1, x_1^0) \\big)$ | \n",
    "| 1 | $f\\big( R(x_1^1, x_0^0) \\big)$ |\n",
    "| 1 | $f\\big( R(x_0^1, x_0^1) \\big)$ |\n",
    "| 1 | $f\\big( R(x_0^1, x_1^1) \\big)$ |\n",
    "| 1 | $f\\big( R(x_1^1, x_1^1) \\big)$ |\n",
    "| 1 | $f\\big( R(x_1^1, x_0^1) \\big)$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6385, -0.1383, -0.6338,  0.6385, -0.1383, -0.6338],\n",
       "          [ 0.6385, -0.1383, -0.6338,  1.0864, -0.5861, -0.2296],\n",
       "          [ 1.0864, -0.5861, -0.2296,  0.6385, -0.1383, -0.6338],\n",
       "          [ 1.0864, -0.5861, -0.2296,  1.0864, -0.5861, -0.2296],\n",
       "          [ 0.6385, -0.1383, -0.6338, -1.5160,  0.4863,  0.1744],\n",
       "          [ 0.6385, -0.1383, -0.6338,  0.1482,  0.0818,  1.0803],\n",
       "          [ 1.0864, -0.5861, -0.2296, -1.5160,  0.4863,  0.1744],\n",
       "          [ 1.0864, -0.5861, -0.2296,  0.1482,  0.0818,  1.0803]],\n",
       "\n",
       "         [[-1.5160,  0.4863,  0.1744,  0.6385, -0.1383, -0.6338],\n",
       "          [-1.5160,  0.4863,  0.1744,  1.0864, -0.5861, -0.2296],\n",
       "          [ 0.1482,  0.0818,  1.0803,  0.6385, -0.1383, -0.6338],\n",
       "          [ 0.1482,  0.0818,  1.0803,  1.0864, -0.5861, -0.2296],\n",
       "          [-1.5160,  0.4863,  0.1744, -1.5160,  0.4863,  0.1744],\n",
       "          [-1.5160,  0.4863,  0.1744,  0.1482,  0.0818,  1.0803],\n",
       "          [ 0.1482,  0.0818,  1.0803, -1.5160,  0.4863,  0.1744],\n",
       "          [ 0.1482,  0.0818,  1.0803,  0.1482,  0.0818,  1.0803]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g., if relation net is identity function, the output is\n",
    "temp.view(1, 2, 2*2*2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`hs` Outputs: torch.Size([5, 2, 6])\n",
      "tensor([[0.0213, 0.0195, 0.0000, 0.0119, 0.0000, 0.0000],\n",
      "        [0.0245, 0.0231, 0.0000, 0.0172, 0.0000, 0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# relation_net\n",
    "hs = model.relation_net(e)  # hs: (B, N, 2H)\n",
    "print(f'`hs` Outputs: {hs.size()}')\n",
    "print(hs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`z` Outputs: torch.Size([5, 2, 3])\n",
      "tensor([[-0.5160,  1.4140,  1.9255],\n",
      "        [ 0.5335,  0.4239, -1.6318]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "`x` Outputs: torch.Size([5, 5])\n",
      "tensor([ 1.5526,  0.4137, -0.5663, -1.4268,  0.0267],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sample: parameters of a probability distribution in a low-dimensional space z for each class\n",
    "z, kld_loss = model.sample(hs, size=model.hidden_size)  # z: (B, N, H)\n",
    "x = l.mean(1)  # x: (B, E)\n",
    "print(f'`z` Outputs: {z.size()}')\n",
    "print(z[0])\n",
    "print()\n",
    "print(f'`x` Outputs: {x.size()}')\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`parameters` Outputs: torch.Size([5, 2, 5])\n",
      "tensor([[-0.4987, -0.0186, -2.2595,  0.5235,  0.0492],\n",
      "        [-2.8540,  0.4142,  1.9802, -0.8921, -0.0442]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# decode\n",
    "parameters = model.decode(z)\n",
    "print(f'`parameters` Outputs: {parameters.size()}')\n",
    "print(parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 4.9740, Accuracy = 0.2000\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "loss, acc = model.predict(x, parameters, s_labels)\n",
    "print(f'Loss = {loss:.4f}, Accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2465, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_inner_step = 5\n",
    "n_finetuning_step = 5\n",
    "\n",
    "s_x, s_z, kld_loss, s_attn = model.forward_encoder(s_inputs, rt_attn=rt_attn)\n",
    "# initialize z', \n",
    "z_prime = s_z\n",
    "train_loss, train_acc, _ = model.forward_decoder(z=z_prime, x=s_x, labels=s_labels)\n",
    "# inner adaptation to z\n",
    "for i in range(n_inner_step):\n",
    "    z_prime.retain_grad()\n",
    "    train_loss.backward(retain_graph=True)\n",
    "    z_prime = z_prime - model.inner_lr * z_prime.grad.data\n",
    "\n",
    "    train_loss, train_acc, parameters = model.forward_decoder(z=z_prime, x=s_x, labels=s_labels)\n",
    "    \n",
    "z_prime = z_prime.detach()  # Stop Gradient\n",
    "z_penalty = torch.mean((z_prime - s_z)**2)\n",
    "print(z_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "ps = list((meta_train.data_dir / 'kdd17/price_long_50').glob('*.csv'))\n",
    "with (Path('../data').resolve() / 'kdd17/stock_universe.json').open('r') as file:\n",
    "    universe_dict = json.load(file)\n",
    "\n",
    "universe_key = 'known'\n",
    "universe = universe_dict['0'][universe_key]\n",
    "iterator = [p for p in ps if p.name.strip('.csv') in universe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = iterator[29]\n",
    "stock_symbol = p.name.rstrip('.csv')\n",
    "df_single = meta_train.load_single_stock(p)\n",
    "df_single = df_single.loc[df_single[\"date\"].between(\"2014-01-01\", '2015-01-01')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = p.name.strip('.csv') # 'AMZN'\n",
    "window_size = 5\n",
    "n_support = 4\n",
    "df_stock = meta_train.data[symbol]\n",
    "labels_indices = meta_train.candidates[symbol]\n",
    "labels_candidates = labels_indices[labels_indices >= window_size]\n",
    "idx = meta_train.get_possible_idx(df_stock, labels_candidates)\n",
    "labels_candidates = labels_candidates[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,   16,   17, ..., 1982, 1983, 1984], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-02-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-02-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-02-16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-02-21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2007-02-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2007-02-23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007-02-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007-02-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2007-02-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2007-03-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007-03-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2007-03-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2007-03-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2007-03-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  label\n",
       "0  2007-02-14      2\n",
       "1  2007-02-15      0\n",
       "2  2007-02-16      2\n",
       "3  2007-02-20      2\n",
       "4  2007-02-21      0\n",
       "5  2007-02-22      2\n",
       "6  2007-02-23      2\n",
       "7  2007-02-26      2\n",
       "8  2007-02-27      0\n",
       "9  2007-02-28      1\n",
       "10 2007-03-01      2\n",
       "11 2007-03-02      0\n",
       "12 2007-03-05      0\n",
       "13 2007-03-06      1\n",
       "14 2007-03-07      2\n",
       "15 2007-03-08      1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.loc[:15, ['date', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_q = np.array([labels_candidates[0]])\n",
    "y_qs = y_q - window_size\n",
    "query, query_labels = meta_train.generate_data(df_stock, y_start=y_qs, y_end=y_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.5848,  0.6405, -1.9772,  0.3073,  0.3073,  0.1392,  0.7574,\n",
       "          0.9728,  1.0874,  0.943 ,  0.9697],\n",
       "        [ 1.1299,  1.5819, -0.113 , -1.4202, -1.4202,  1.1243,  1.9407,\n",
       "          2.2467,  2.4124,  2.2888,  2.3452],\n",
       "        [ 0.8847,  1.0845, -0.1142, -1.017 , -1.017 ,  1.387 ,  2.5942,\n",
       "          3.0403,  3.2848,  3.2297,  3.273 ],\n",
       "        [-0.5072,  0.3099, -1.0989,  1.2842,  1.2842,  0.1071,  0.9862,\n",
       "          1.5892,  1.8456,  1.8785,  1.8921],\n",
       "        [ 0.    ,  0.6787, -0.2828, -0.3663, -0.3663,  0.2262,  1.0775,\n",
       "          1.744 ,  2.0475,  2.1993,  2.1752]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>zd5</th>\n",
       "      <th>zd10</th>\n",
       "      <th>zd15</th>\n",
       "      <th>zd20</th>\n",
       "      <th>zd25</th>\n",
       "      <th>zd30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>-0.584793</td>\n",
       "      <td>0.640487</td>\n",
       "      <td>-1.977162</td>\n",
       "      <td>0.307265</td>\n",
       "      <td>0.307266</td>\n",
       "      <td>0.139237</td>\n",
       "      <td>0.757449</td>\n",
       "      <td>0.972802</td>\n",
       "      <td>1.087442</td>\n",
       "      <td>0.942961</td>\n",
       "      <td>0.969704</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2007-03-02</td>\n",
       "      <td>1.129935</td>\n",
       "      <td>1.581912</td>\n",
       "      <td>-0.112997</td>\n",
       "      <td>-1.420212</td>\n",
       "      <td>-1.420211</td>\n",
       "      <td>1.124289</td>\n",
       "      <td>1.940672</td>\n",
       "      <td>2.246698</td>\n",
       "      <td>2.412423</td>\n",
       "      <td>2.288781</td>\n",
       "      <td>2.345161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007-03-05</td>\n",
       "      <td>0.884695</td>\n",
       "      <td>1.084466</td>\n",
       "      <td>-0.114158</td>\n",
       "      <td>-1.016952</td>\n",
       "      <td>-1.016952</td>\n",
       "      <td>1.386984</td>\n",
       "      <td>2.594176</td>\n",
       "      <td>3.040332</td>\n",
       "      <td>3.284815</td>\n",
       "      <td>3.229709</td>\n",
       "      <td>3.273000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2007-03-06</td>\n",
       "      <td>-0.507188</td>\n",
       "      <td>0.309935</td>\n",
       "      <td>-1.098912</td>\n",
       "      <td>1.284249</td>\n",
       "      <td>1.284248</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.986189</td>\n",
       "      <td>1.589176</td>\n",
       "      <td>1.845586</td>\n",
       "      <td>1.878530</td>\n",
       "      <td>1.892075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2007-03-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678725</td>\n",
       "      <td>-0.282814</td>\n",
       "      <td>-0.366303</td>\n",
       "      <td>-0.366304</td>\n",
       "      <td>0.226246</td>\n",
       "      <td>1.077489</td>\n",
       "      <td>1.743966</td>\n",
       "      <td>2.047510</td>\n",
       "      <td>2.199321</td>\n",
       "      <td>2.175243</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close  adj_close       zd5  \\\n",
       "10 2007-03-01 -0.584793  0.640487 -1.977162  0.307265   0.307266  0.139237   \n",
       "11 2007-03-02  1.129935  1.581912 -0.112997 -1.420212  -1.420211  1.124289   \n",
       "12 2007-03-05  0.884695  1.084466 -0.114158 -1.016952  -1.016952  1.386984   \n",
       "13 2007-03-06 -0.507188  0.309935 -1.098912  1.284249   1.284248  0.107070   \n",
       "14 2007-03-07  0.000000  0.678725 -0.282814 -0.366303  -0.366304  0.226246   \n",
       "\n",
       "        zd10      zd15      zd20      zd25      zd30  label  \n",
       "10  0.757449  0.972802  1.087442  0.942961  0.969704      2  \n",
       "11  1.940672  2.246698  2.412423  2.288781  2.345161      0  \n",
       "12  2.594176  3.040332  3.284815  3.229709  3.273000      0  \n",
       "13  0.986189  1.589176  1.845586  1.878530  1.892075      1  \n",
       "14  1.077489  1.743966  2.047510  2.199321  2.175243      2  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.loc[10:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-02-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-02-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-02-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-02-21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2007-02-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2007-02-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  label\n",
       "0 2007-02-14      2\n",
       "1 2007-02-15      0\n",
       "2 2007-02-16      1\n",
       "3 2007-02-20      0\n",
       "4 2007-02-21      2\n",
       "5 2007-02-22      2\n",
       "6 2007-02-23      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.loc[:6, ['date', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   6,    7,    8, ..., 1982, 1983, 1984], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_idx(df_stock, labels_candidates):\n",
    "    i = 0\n",
    "    while i < len(labels_candidates):\n",
    "        rise, fall = get_rise_fall(df_stock, labels_candidates, idx=i)\n",
    "        if len(rise) + len(fall) == 4:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "def get_rise_fall(df_stock, labels_candidates, idx):\n",
    "    df_check = df_stock.loc[labels_candidates[:idx], 'label'].sort_index(ascending=False)\n",
    "    rise = df_check.index[df_check == meta_train.labels_dict['rise']][:(n_support // 2)].to_numpy()\n",
    "    fall = df_check.index[df_check == meta_train.labels_dict['fall']][:(n_support // 2)].to_numpy()\n",
    "    return rise, fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unpossible candidates\n",
    "idx = get_possible_idx(df_stock, labels_candidates)\n",
    "labels_candidates = labels_candidates[idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1277],\n",
       "       [ 806],\n",
       "       [ 407],\n",
       "       [1164],\n",
       "       [  66]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_q = np.array(np.random.choice(labels_candidates, size=(5,), replace=False))\n",
    "y_qs = y_q - window_size\n",
    "query, query_labels = meta_train.generate_data(df_stock, y_start=y_qs, y_end=y_q)\n",
    "support = []\n",
    "support_labels = []\n",
    "for q in y_q:\n",
    "    q_idx = np.arange(len(labels_candidates))[labels_candidates == q][0]\n",
    "    rise, fall = get_rise_fall(df_stock, labels_candidates, idx=q_idx)\n",
    "    y_s = np.concatenate([fall, rise])\n",
    "    y_ss = y_s - window_size\n",
    "    data_s, label_s = meta_train.generate_data(df_stock, y_start=y_ss, y_end=y_s)\n",
    "    data_s = np.array(data_s)\n",
    "    support.append(data_s)\n",
    "    support_labels.append(label_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "for x in np.expand_dims(query_labels, 1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 5, 11)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(support).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = y_q[0]\n",
    "q_idx = np.arange(len(labels_candidates))[labels_candidates == q][0]\n",
    "rise, fall = get_rise_fall(df_stock, labels_candidates, idx=q_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = np.concatenate([fall, rise])\n",
    "y_ss = y_s - window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, support_labels = meta_train.generate_data(df_stock, y_start=y_ss, y_end=y_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 241\n"
     ]
    }
   ],
   "source": [
    "symbol = 'AMZN'\n",
    "window_size = 5\n",
    "n_shot = 2\n",
    "df_stock = meta_train.data[symbol]\n",
    "labels_indices = meta_train.candidates[symbol]\n",
    "y_cand = labels_indices[labels_indices >= window_size]\n",
    "n_rise = 0\n",
    "n_fall = 0\n",
    "support= []\n",
    "support_sample = []\n",
    "query = []\n",
    "support_turn = True\n",
    "query_turn = False\n",
    "query_sample = []\n",
    "for idx in y_cand:\n",
    "\n",
    "    # ex. k = 2\n",
    "    if support_turn and  n_rise < n_shot or n_fall < n_shot:\n",
    "        if n_rise < 2 and df_stock['label'][idx] == 1:\n",
    "            n_rise +=1\n",
    "            support_sample.append(idx)\n",
    "        elif n_fall < 2 and df_stock['label'][idx] == 0:\n",
    "            n_fall +=1\n",
    "            support_sample.append(idx)\n",
    "        continue\n",
    "\n",
    "    if n_rise == n_shot and n_fall == n_shot:\n",
    "        support.append(support_sample)\n",
    "        support_sample = []\n",
    "        n_rise = 0\n",
    "        n_fall = 0\n",
    "        query_turn = True\n",
    "        support_turn = False \n",
    "\n",
    "    if query_turn:\n",
    "        query_sample.append(idx)\n",
    "        query.append(query_sample)\n",
    "        query_sample = []\n",
    "        query_turn = False\n",
    "        support_turn = True\n",
    "        continue\n",
    "support_idx_set = np.array(support)\n",
    "query_idx_set = np.array(query)\n",
    "print(len(support_idx_set), len(query_idx_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14],\n",
       "       [  20],\n",
       "       [  28],\n",
       "       [  52],\n",
       "       [  57],\n",
       "       [  63],\n",
       "       [  71],\n",
       "       [  77],\n",
       "       [  83],\n",
       "       [  89],\n",
       "       [  95],\n",
       "       [ 102],\n",
       "       [ 112],\n",
       "       [ 121],\n",
       "       [ 128],\n",
       "       [ 135],\n",
       "       [ 144],\n",
       "       [ 162],\n",
       "       [ 168],\n",
       "       [ 174],\n",
       "       [ 181],\n",
       "       [ 190],\n",
       "       [ 196],\n",
       "       [ 205],\n",
       "       [ 211],\n",
       "       [ 222],\n",
       "       [ 239],\n",
       "       [ 248],\n",
       "       [ 253],\n",
       "       [ 258],\n",
       "       [ 268],\n",
       "       [ 274],\n",
       "       [ 279],\n",
       "       [ 284],\n",
       "       [ 291],\n",
       "       [ 299],\n",
       "       [ 304],\n",
       "       [ 315],\n",
       "       [ 321],\n",
       "       [ 328],\n",
       "       [ 336],\n",
       "       [ 341],\n",
       "       [ 350],\n",
       "       [ 357],\n",
       "       [ 364],\n",
       "       [ 372],\n",
       "       [ 377],\n",
       "       [ 383],\n",
       "       [ 389],\n",
       "       [ 394],\n",
       "       [ 401],\n",
       "       [ 406],\n",
       "       [ 411],\n",
       "       [ 420],\n",
       "       [ 427],\n",
       "       [ 435],\n",
       "       [ 443],\n",
       "       [ 450],\n",
       "       [ 455],\n",
       "       [ 462],\n",
       "       [ 467],\n",
       "       [ 473],\n",
       "       [ 479],\n",
       "       [ 488],\n",
       "       [ 493],\n",
       "       [ 502],\n",
       "       [ 509],\n",
       "       [ 514],\n",
       "       [ 521],\n",
       "       [ 527],\n",
       "       [ 533],\n",
       "       [ 544],\n",
       "       [ 549],\n",
       "       [ 555],\n",
       "       [ 562],\n",
       "       [ 567],\n",
       "       [ 575],\n",
       "       [ 585],\n",
       "       [ 592],\n",
       "       [ 598],\n",
       "       [ 605],\n",
       "       [ 617],\n",
       "       [ 623],\n",
       "       [ 629],\n",
       "       [ 634],\n",
       "       [ 647],\n",
       "       [ 659],\n",
       "       [ 664],\n",
       "       [ 671],\n",
       "       [ 680],\n",
       "       [ 689],\n",
       "       [ 700],\n",
       "       [ 708],\n",
       "       [ 719],\n",
       "       [ 728],\n",
       "       [ 735],\n",
       "       [ 744],\n",
       "       [ 752],\n",
       "       [ 761],\n",
       "       [ 772],\n",
       "       [ 779],\n",
       "       [ 787],\n",
       "       [ 793],\n",
       "       [ 800],\n",
       "       [ 806],\n",
       "       [ 812],\n",
       "       [ 817],\n",
       "       [ 826],\n",
       "       [ 831],\n",
       "       [ 839],\n",
       "       [ 851],\n",
       "       [ 861],\n",
       "       [ 868],\n",
       "       [ 882],\n",
       "       [ 888],\n",
       "       [ 893],\n",
       "       [ 914],\n",
       "       [ 919],\n",
       "       [ 926],\n",
       "       [ 935],\n",
       "       [ 943],\n",
       "       [ 952],\n",
       "       [ 963],\n",
       "       [ 975],\n",
       "       [ 987],\n",
       "       [ 992],\n",
       "       [1000],\n",
       "       [1011],\n",
       "       [1021],\n",
       "       [1027],\n",
       "       [1035],\n",
       "       [1045],\n",
       "       [1055],\n",
       "       [1062],\n",
       "       [1071],\n",
       "       [1078],\n",
       "       [1085],\n",
       "       [1093],\n",
       "       [1100],\n",
       "       [1111],\n",
       "       [1116],\n",
       "       [1122],\n",
       "       [1131],\n",
       "       [1137],\n",
       "       [1144],\n",
       "       [1149],\n",
       "       [1154],\n",
       "       [1161],\n",
       "       [1167],\n",
       "       [1179],\n",
       "       [1185],\n",
       "       [1190],\n",
       "       [1198],\n",
       "       [1208],\n",
       "       [1214],\n",
       "       [1222],\n",
       "       [1227],\n",
       "       [1235],\n",
       "       [1244],\n",
       "       [1252],\n",
       "       [1262],\n",
       "       [1270],\n",
       "       [1279],\n",
       "       [1291],\n",
       "       [1301],\n",
       "       [1307],\n",
       "       [1318],\n",
       "       [1326],\n",
       "       [1331],\n",
       "       [1336],\n",
       "       [1346],\n",
       "       [1352],\n",
       "       [1365],\n",
       "       [1372],\n",
       "       [1377],\n",
       "       [1384],\n",
       "       [1396],\n",
       "       [1405],\n",
       "       [1414],\n",
       "       [1422],\n",
       "       [1432],\n",
       "       [1443],\n",
       "       [1453],\n",
       "       [1467],\n",
       "       [1474],\n",
       "       [1481],\n",
       "       [1492],\n",
       "       [1497],\n",
       "       [1505],\n",
       "       [1510],\n",
       "       [1521],\n",
       "       [1528],\n",
       "       [1539],\n",
       "       [1547],\n",
       "       [1554],\n",
       "       [1562],\n",
       "       [1567],\n",
       "       [1575],\n",
       "       [1585],\n",
       "       [1593],\n",
       "       [1598],\n",
       "       [1619],\n",
       "       [1624],\n",
       "       [1644],\n",
       "       [1650],\n",
       "       [1664],\n",
       "       [1671],\n",
       "       [1677],\n",
       "       [1684],\n",
       "       [1689],\n",
       "       [1698],\n",
       "       [1705],\n",
       "       [1718],\n",
       "       [1729],\n",
       "       [1737],\n",
       "       [1745],\n",
       "       [1753],\n",
       "       [1762],\n",
       "       [1769],\n",
       "       [1786],\n",
       "       [1796],\n",
       "       [1801],\n",
       "       [1810],\n",
       "       [1815],\n",
       "       [1822],\n",
       "       [1828],\n",
       "       [1839],\n",
       "       [1847],\n",
       "       [1852],\n",
       "       [1861],\n",
       "       [1869],\n",
       "       [1883],\n",
       "       [1900],\n",
       "       [1911],\n",
       "       [1921],\n",
       "       [1928],\n",
       "       [1935],\n",
       "       [1942],\n",
       "       [1949],\n",
       "       [1965],\n",
       "       [1977]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_indices = self.candidates[symbol]\n",
    "labels_candidates = labels_indices[labels_indices >= window_size]\n",
    "y_s = np.array(sorted(np.random.choice(labels_candidates, size=(self.n_sample,), replace=False)))\n",
    "y_ss = y_s-window_size\n",
    "support, support_labels = self.generate_data(df_stock, y_start=y_ss, y_end=y_s)\n",
    "\n",
    "# code for jumpped tags like [1(support), 0, 0, 1(query)]\n",
    "# y_q = labels_indices[np.arange(len(labels_indices))[np.isin(labels_indices, y_s)] + self.n_lag]\n",
    "y_q = y_s + self.n_lag\n",
    "y_qs = y_s - window_size if self.keep_support_history else y_q - window_size\n",
    "query, query_labels = self.generate_data(df_stock, y_start=y_qs, y_end=y_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SMILE-YJBuims-')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20751be6a0cf8c615e71af54ada580fb04d2b8017a238485cee7184b2ef75c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

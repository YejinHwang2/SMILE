{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "main_path = Path('..').resolve()\n",
    "sys.path.append(str(main_path))\n",
    "\n",
    "import seaborn as sns\n",
    "from src.dataset import MetaStockDataset\n",
    "from src.utils import ARGProcessor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data and candidates for train: 100%|██████████| 35/35 [00:00<00:00, 45.50it/s]\n",
      "Processing data and candidates for valid-time: 100%|██████████| 35/35 [00:00<00:00, 81.01it/s]\n",
      "Processing data and candidates for valid-stock: 100%|██████████| 10/10 [00:00<00:00, 59.98it/s]\n",
      "Processing data and candidates for valid-mix: 100%|██████████| 10/10 [00:00<00:00, 55.44it/s]\n",
      "Processing data and candidates for test-time: 100%|██████████| 35/35 [00:00<00:00, 82.57it/s]\n",
      "Processing data and candidates for test-stock: 100%|██████████| 5/5 [00:00<00:00, 54.08it/s]\n",
      "Processing data and candidates for test-mix: 100%|██████████| 5/5 [00:00<00:00, 73.43it/s]\n"
     ]
    }
   ],
   "source": [
    "setting_file = Path('.') / 'kdd.yml'\n",
    "\n",
    "meta_args = ARGProcessor(setting_file=setting_file)\n",
    "data_kwargs = meta_args.get_args(cls=MetaStockDataset)\n",
    "\n",
    "meta_train = MetaStockDataset(meta_type='train', **data_kwargs)\n",
    "meta_valid_time = MetaStockDataset(meta_type='valid-time', **data_kwargs)\n",
    "meta_valid_stock = MetaStockDataset(meta_type='valid-stock', **data_kwargs)\n",
    "meta_valid_mix = MetaStockDataset(meta_type='valid-mix', **data_kwargs)\n",
    "meta_test_time = MetaStockDataset(meta_type='test-time', **data_kwargs)\n",
    "meta_test_stock = MetaStockDataset(meta_type='test-stock', **data_kwargs)\n",
    "meta_test_mix = MetaStockDataset(meta_type='test-mix', **data_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = meta_train.generate_tasks()\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to('cpu')\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.numpy()\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['query'][..., 0].reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train.meta_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_density(ds):\n",
    "    all_data = ds.generate_tasks()\n",
    "    fig, axes = plt.subplots(11, 2, figsize=(10, 16))\n",
    "    for i in range(11):\n",
    "        for t in range(2):\n",
    "            \n",
    "            f1_q = all_data['query'][..., i].reshape(-1)\n",
    "            f1_s = all_data['support'][:, t, :, i].reshape(-1)\n",
    "            sns.histplot(data=f1_q, ax=axes[i, t], color=\"blue\", label='query', alpha=0.2)\n",
    "            sns.histplot(data=f1_s, ax=axes[i, t], color=\"red\", label='support', alpha=0.2)\n",
    "            axes[i, t].legend()\n",
    "            if i == 0:\n",
    "                axes[i, t].set_title(f'Class: {t}')\n",
    "    fig.suptitle(f'{ds.meta_type}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_density(ds=meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_density(ds=meta_valid_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_density(ds=meta_valid_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_labels(meta_ds):\n",
    "    cnts = Counter()\n",
    "    for s in meta_ds.symbols:\n",
    "        t = meta_ds.data[s].loc[meta_ds.candidates[s], 'label'].value_counts().to_dict()\n",
    "        cnts.update(t)\n",
    "    return cnts\n",
    "\n",
    "cnt_data = {'ds': [], 'n_stock': [], 'fall': [], 'rise': []}\n",
    "for ds in [meta_train, meta_valid_time, meta_valid_stock, meta_valid_mix, meta_test_time, meta_test_stock, meta_test_mix]:\n",
    "    cnts = count_labels(ds)\n",
    "    cnt_data['ds'].append(ds.meta_type)\n",
    "    cnt_data['n_stock'].append(len(ds.symbols))\n",
    "    cnt_data['fall'].append(cnts[0])\n",
    "    cnt_data['rise'].append(cnts[1])\n",
    "df_cnt = pd.DataFrame(cnt_data)\n",
    "df_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q dist\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_q_dist(meta_dataset):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    idx = np.arange(max(meta_dataset.q_dist.keys()))\n",
    "    values = [meta_dataset.q_dist[i] if meta_dataset.q_dist.get(i) else 0 for i in idx]\n",
    "\n",
    "    ax.bar(idx, values)\n",
    "    ax.set_xlabel('Query index in labels')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Meta Type: {meta_dataset.meta_type}')\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train.reset_q_idx_dist()\n",
    "n = 1000\n",
    "for i in tqdm(range(n), total=n):\n",
    "    meta_train.generate_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_q_dist(meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train.data['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./test_writer')\n",
    "writer.add_figure('b', fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check Time is Enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train.n_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time(ds, n_support=10):\n",
    "    window_size = 15\n",
    "    ds.n_support = n_support\n",
    "    cnt = 0\n",
    "    for symbol in ds.symbols:\n",
    "        df_stock = ds.data[symbol]\n",
    "        labels_indices = ds.candidates[symbol] \n",
    "        labels_indices = labels_indices[labels_indices >= window_size]\n",
    "\n",
    "        for i in range(len(labels_indices)):\n",
    "            array = df_stock.loc[labels_indices, 'label'].loc[:(labels_indices[i])].to_numpy()\n",
    "            if ds.check_condition(array):\n",
    "                break\n",
    "        if i == len(labels_indices)-1:\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = [meta_train, meta_valid_time, meta_valid_stock, meta_valid_mix, meta_test_time, meta_test_stock, meta_test_mix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train(35) 0\n",
      "valid-time(35) 0\n",
      "valid-stock(10) 0\n",
      "valid-mix(10) 0\n",
      "test-time(35) 0\n",
      "test-stock(5) 0\n",
      "test-mix(5) 0\n"
     ]
    }
   ],
   "source": [
    "for ds in ds_list:\n",
    "    print(f'{ds.meta_type}({len(ds.symbols)})', test_time(ds, n_support=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "symbol = 'AAPL'\n",
    "df_stock = meta_train.data[symbol]\n",
    "# filter out unpossible candidates\n",
    "labels_indices = meta_train.candidates[symbol] \n",
    "labels_indices = labels_indices[labels_indices >= window_size]\n",
    "\n",
    "for i in range(len(labels_indices)):\n",
    "    array = df_stock.loc[labels_indices, 'label'].loc[:(labels_indices[i])].to_numpy()\n",
    "    if meta_train.check_condition(array):\n",
    "        break\n",
    "\n",
    "# satisfied condition label index | smallest support index | smallest query index\n",
    "# candidates = labels_indices[(i+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train.n_support, meta_train.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df_stock.loc[labels_indices, 'label'].loc[:(labels_indices[29])].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = array.sum() >= meta_train.n_classes\n",
    "cond2 = np.isin(array, meta_train.labels_dict['fall']).sum() >= meta_train.n_support\n",
    "cond3 = np.isin(array, meta_train.labels_dict['rise']).sum() >= meta_train.n_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_indices, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(candidates), len(df_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.loc[labels_indices].iloc[:10, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    query = None,\n",
    "    query_labels = None,\n",
    "    support = None,\n",
    "    support_labels = None,\n",
    ")\n",
    "\n",
    "q_target = np.random.choice(candidates)   # index in the dataframe\n",
    "# for q_target in y_q:\n",
    "    # Queries\n",
    "q_idx = np.arange(len(labels_indices))[labels_indices == q_target][0]  # get the index of label data\n",
    "q_end = np.array([q_target]) \n",
    "q_start = q_end - window_size\n",
    "q_data, q_labels = meta_train.generate_data(df_stock, y_start=q_start, y_end=q_end)\n",
    "\n",
    "data['query'] = q_data\n",
    "data['query_labels'] = q_labels[0]  # (1,)\n",
    "\n",
    "# Supports\n",
    "s_fall, s_rise = meta_train.get_rise_fall(df_stock, labels_indices, idx=q_idx, n_select=meta_train.n_support)\n",
    "s_end = np.concatenate([s_fall, s_rise])\n",
    "s_start = s_end - window_size\n",
    "s_data, s_labels = meta_train.generate_data(df_stock, y_start=s_start, y_end=s_end)\n",
    "\n",
    "data['support'] = s_data\n",
    "data['support_labels'] = s_labels  # (N*K,)\n",
    "\n",
    "print()   \n",
    "print(f'query index: {q_idx}({q_target}) = {df_stock.loc[q_target, \"label\"]}')\n",
    "print(f'- start={q_start} end={q_end}')\n",
    "print(f'support indices:')\n",
    "print(f'- start={s_start} end={s_end}')\n",
    "print(f'{df_stock.loc[s_end, \"label\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check queries distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "window_size = 10\n",
    "def get_q_label_dist(ds):\n",
    "    q_label_dist = Counter()\n",
    "    for symbol in ds.symbols:\n",
    "        df_stock = ds.data[symbol]\n",
    "        # filter out unpossible candidates\n",
    "        labels_indices = ds.candidates[symbol] \n",
    "        labels_indices = labels_indices[labels_indices >= window_size]\n",
    "\n",
    "        for i in range(len(labels_indices)):\n",
    "            array = df_stock.loc[labels_indices, 'label'].loc[:(labels_indices[i])].to_numpy()\n",
    "            if ds.check_condition(array):\n",
    "                break\n",
    "        candidates = labels_indices[(i+1):]  # query candidates\n",
    "        \n",
    "        counts = df_stock.loc[candidates, 'label'].value_counts().to_dict()\n",
    "        q_label_dist.update(counts)\n",
    "    \n",
    "    return q_label_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_label_dists = {'type': [], 'fall': [], 'rise': []}\n",
    "for ds in [meta_train, meta_valid_time, meta_valid_stock, meta_valid_mix, \n",
    "    meta_test_time, meta_test_stock, meta_test_mix]:\n",
    "    q_label_dist = get_q_label_dist(ds)\n",
    "    q_label_dists['type'].append(ds.meta_type)\n",
    "    q_label_dists['fall'].append(q_label_dist[0])\n",
    "    q_label_dists['rise'].append(q_label_dist[1])\n",
    "\n",
    "q_label_dists = pd.DataFrame(q_label_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_label_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.model import MetaModel\n",
    "\n",
    "model_kwargs = meta_args.get_args(cls=MetaModel)\n",
    "model = MetaModel(**model_kwargs)\n",
    "\n",
    "rt_attn = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_lstm\n",
    "l, attn = model.encode_lstm(s_inputs, rt_attn=rt_attn)  # lstm_encoded: (B, N*K, E)\n",
    "print(f'`l` Outputs: {l.size()}, {attn.size()}')\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if isinstance(attn, torch.Tensor):\n",
    "    attn_numpy = attn.detach().numpy()\n",
    "else:\n",
    "    attn_numpy = attn\n",
    "masks = [0, 0, 1, 1]\n",
    "\n",
    "B = attn_numpy.shape[0]\n",
    "fig, axes = plt.subplots(1, B, figsize=(12, 10))\n",
    "for i in range(B):\n",
    "    ax = axes[i]\n",
    "    ax.matshow(attn_numpy[i])\n",
    "    ax.set_title(f'Data {i}')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yticks(np.arange(len(masks)))\n",
    "    ax.set_yticklabels(masks)\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_xlabel('Time Stamp')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_linear\n",
    "# Reshape the size\n",
    "B = l.size(0)\n",
    "N = model.output_size\n",
    "K = l.size(1) // N\n",
    "if rt_attn:\n",
    "    attn = attn.view(B, N, K, -1)  # attn: (B, N, K, T)\n",
    "l_reshape = l.view(B, N, K, -1)  # l_reshape: (B, N, K, E)\n",
    "e = model.encoder(l_reshape)  # e: (B, N, K, H)\n",
    "print(f'`encoded` Outputs: {e.size()}')\n",
    "print(e[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Net: class-conditional multivariate Gaussian distribution with a diagonal covariance\n",
    "\n",
    "The paper concatenate tensors for relation net inputs.\n",
    "\n",
    "Let $R(x_{i}^{p}, x_{j}^{q})$ to represent the inputs of hidden state on concatenated relations between classes, $i, j$ for shot index, $p, q$ for class index.\n",
    "\n",
    "The tensor shape is $(B, N^2, K^2, 2H)$. For each data(row) in $B$, the data relationship is $\\sum_{i, j}^N \\sum_{p, q}^{K} R(x_{i}^{p}, x_{j}^{q})$\n",
    "\n",
    "e.g.,  N way K shot = 2 way 2 shot\n",
    "\n",
    "| Relation | Left | Right |\n",
    "|---|---|---|\n",
    "| $R(x_0^0, x_0^0)$ | $h_{K_0}^{N_0}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_0^0, x_1^0)$ | $h_{K_0}^{N_0}$ | $h_{K_1}^{N_0}$ | \n",
    "| $R(x_1^0, x_1^0)$ | $h_{K_1}^{N_0}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_1^0, x_0^0)$ | $h_{K_1}^{N_0}$ | $h_{K_1}^{N_0}$ | \n",
    "| | | |\n",
    "| $R(x_0^0, x_0^1)$ | $h_{K_0}^{N_0}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_0^0, x_1^1)$ | $h_{K_0}^{N_0}$ | $h_{K_1}^{N_1}$ | \n",
    "| $R(x_1^0, x_1^1)$ | $h_{K_1}^{N_0}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_1^0, x_0^1)$ | $h_{K_1}^{N_0}$ | $h_{K_1}^{N_1}$ | \n",
    "| | | |\n",
    "| $R(x_0^1, x_0^0)$ | $h_{K_0}^{N_1}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_0^1, x_1^0)$ | $h_{K_0}^{N_1}$ | $h_{K_1}^{N_0}$ | \n",
    "| $R(x_1^1, x_1^0)$ | $h_{K_1}^{N_1}$ | $h_{K_0}^{N_0}$ |\n",
    "| $R(x_1^1, x_0^0)$ | $h_{K_1}^{N_1}$ | $h_{K_1}^{N_0}$ | \n",
    "| | | |\n",
    "| $R(x_0^1, x_0^1)$ | $h_{K_0}^{N_1}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_0^1, x_1^1)$ | $h_{K_0}^{N_1}$ | $h_{K_1}^{N_1}$ | \n",
    "| $R(x_1^1, x_1^1)$ | $h_{K_1}^{N_1}$ | $h_{K_0}^{N_1}$ |\n",
    "| $R(x_1^1, x_0^1)$ | $h_{K_1}^{N_1}$ | $h_{K_1}^{N_1}$ | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g.\n",
    "a = torch.randn(1, 2, 2, 3)\n",
    "left = torch.repeat_interleave(a, 2, dim=2)\n",
    "left = torch.repeat_interleave(left, 2, dim=1)\n",
    "right = a.repeat((1, 2, 2, 1))\n",
    "temp = torch.cat([left, right], dim=-1)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after relation network, average the values for each class for all shots($K$)\n",
    "\n",
    "e.g.,  N way K shot = 2 way 2 shot\n",
    "\n",
    "| Class | Relation |\n",
    "|---|---|\n",
    "| 0 | $f\\big( R(x_0^0, x_0^0) \\big)$ |\n",
    "| 0 | $f\\big( R(x_0^0, x_1^0) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_1^0) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_0^0) \\big)$ | \n",
    "| 0 | $f\\big( R(x_0^0, x_0^1) \\big)$ |\n",
    "| 0 | $f\\big( R(x_0^0, x_1^1) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_1^1) \\big)$ |\n",
    "| 0 | $f\\big( R(x_1^0, x_0^1) \\big)$ |\n",
    "|   | |\n",
    "| 1 | $f\\big( R(x_0^1, x_0^0) \\big)$ | \n",
    "| 1 | $f\\big( R(x_0^1, x_1^0) \\big)$ | \n",
    "| 1 | $f\\big( R(x_1^1, x_1^0) \\big)$ | \n",
    "| 1 | $f\\big( R(x_1^1, x_0^0) \\big)$ |\n",
    "| 1 | $f\\big( R(x_0^1, x_0^1) \\big)$ |\n",
    "| 1 | $f\\big( R(x_0^1, x_1^1) \\big)$ |\n",
    "| 1 | $f\\big( R(x_1^1, x_1^1) \\big)$ |\n",
    "| 1 | $f\\big( R(x_1^1, x_0^1) \\big)$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., if relation net is identity function, the output is\n",
    "temp.view(1, 2, 2*2*2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_net\n",
    "hs = model.relation_net(e)  # hs: (B, N, 2H)\n",
    "print(f'`hs` Outputs: {hs.size()}')\n",
    "print(hs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample: parameters of a probability distribution in a low-dimensional space z for each class\n",
    "z, kld_loss = model.sample(hs, size=model.hidden_size)  # z: (B, N, H)\n",
    "x = l.mean(1)  # x: (B, E)\n",
    "print(f'`z` Outputs: {z.size()}')\n",
    "print(z[0])\n",
    "print()\n",
    "print(f'`x` Outputs: {x.size()}')\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "parameters = model.decode(z)\n",
    "print(f'`parameters` Outputs: {parameters.size()}')\n",
    "print(parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "loss, score = model.predict(x, parameters, s_labels)\n",
    "print(f'Loss = {loss:.4f}\\nScores =\\n{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, q_scores, s_attn, q_attn = model(\n",
    "    data=data,\n",
    "    rt_attn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_l, s_z, kld_loss, s_attn = model.forward_encoder(s_inputs, rt_attn=rt_attn)\n",
    "\n",
    "# initialize z', Forward Decoder\n",
    "z_prime = s_z\n",
    "s_loss, s_scores, parameters = model.forward_decoder(z=z_prime, l=s_l, labels=s_labels)\n",
    "# inner adaptation to z\n",
    "for i in range(5):\n",
    "    z_prime.retain_grad()\n",
    "    s_loss.backward(retain_graph=True)\n",
    "    z_prime = z_prime - model.inner_lr * z_prime.grad.data\n",
    "    s_loss, s_scores, parameters = model.forward_decoder(z=z_prime, l=s_l, labels=s_labels)\n",
    "\n",
    "# Stop Gradient: \n",
    "# z_prime.requires_grad == False\n",
    "# s_z.requires_grad == True\n",
    "z_prime = z_prime.detach()  \n",
    "z_loss = torch.mean((z_prime - s_z)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss_fn(s_scores, s_labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recorder.update('Support_Accuracy', s_scores, s_labels.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torchmetrics as tm\n",
    "from typing import Dict, Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)\n",
    "\n",
    "t = all_data['query_labels']\n",
    "o = torch.randint(0, 2, size=t.size())\n",
    "# o = torch.rand(size=t.size())\n",
    "y_true = pd.Series(t.numpy(), name='Actual') \n",
    "y_pred = pd.Series(o.numpy(), name='Pred')\n",
    "df_confusion = pd.crosstab(y_true, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall \n",
    "df_confusion = pd.crosstab(y_true, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "precision = df_confusion.values.diagonal() / df_confusion.sum(0)\n",
    "recall = df_confusion.values.diagonal() / df_confusion.sum(1)\n",
    "print(precision.rename('Precision'))\n",
    "print(recall.rename('Recall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_tm = tm.Precision(num_classes=2, average=None)\n",
    "p = precision_tm(o, t)\n",
    "\n",
    "recall_tm = tm.Recall(num_classes=2, average=None)\n",
    "r = recall_tm(o, t)\n",
    "\n",
    "print(p)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricRecorder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        cs = tm.MetricCollection({\n",
    "            'Accuracy': tm.Accuracy(), \n",
    "            'Precision': tm.Precision(num_classes=2, average=None), \n",
    "            'Recall': tm.Recall(num_classes=2, average=None), \n",
    "            'Loss': tm.SumMetric()\n",
    "        })\n",
    "        self.metrics = tm.MetricCollection([\n",
    "            cs.clone('Support_'), cs.clone('Query_'), cs.clone('Finetune_'),\n",
    "            tm.MetricCollection({\n",
    "                'Inner': tm.MeanMetric(), 'Finetuning': tm.MeanMetric()\n",
    "            }, postfix='_LR'),\n",
    "            tm.MetricCollection({\n",
    "                'Total': tm.SumMetric(), \n",
    "                'KLD': tm.SumMetric(), \n",
    "                'Z': tm.SumMetric(),\n",
    "                'Orthogonality': tm.SumMetric()\n",
    "            }, postfix='_Loss')\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.metrics.keys())\n",
    "\n",
    "    def update(self, key, scores=None | torch.FloatTensor, targets=None | torch.LongTensor):\n",
    "        if key.split('_')[-1] in ['Accuracy', 'Precision', 'Recall']:\n",
    "            if targets is None:\n",
    "                raise KeyError('Must insert `targets` to calculate accuracy.')\n",
    "            self.metrics[key].update(scores, targets)\n",
    "        else:\n",
    "            self.metrics[key].update(scores)\n",
    "\n",
    "    def compute(self, prefix: str):\n",
    "        results = {}\n",
    "        for k in self.keys:\n",
    "            m = self.metrics[k].compute()\n",
    "            if isinstance(m, torch.Tensor):\n",
    "                m = m.cpu().detach().numpy()\n",
    "            results[f'{prefix}-{k}'] = m\n",
    "        return results\n",
    "\n",
    "    def reset(self):\n",
    "        for k in self.keys:\n",
    "            self.metrics[k].reset()\n",
    "\n",
    "    def extract_query_loss_acc(self, logs: Dict[str, float] | List[Dict[str, float]]) -> Dict[str, Tuple[float, float]]:\n",
    "        to_filter = ['Query_Accuracy', 'Query_Loss']\n",
    "        check_func = lambda x: sum([1 if f in x[0] else 0 for f in to_filter if f in x[0]])\n",
    "        if isinstance(logs, dict):\n",
    "            # cumulated logs\n",
    "            filtered = dict(filter(check_func, logs.items()))\n",
    "        else:\n",
    "            filtered = {}\n",
    "            for l in logs:\n",
    "                win_filtered = dict(filter(check_func, l.items()))\n",
    "                filtered.update(win_filtered)\n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = MetricRecorder()\n",
    "recorder.reset()  \n",
    "print(recorder.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.model import MetaModel\n",
    "\n",
    "model_kwargs = meta_args.get_args(cls=MetaModel)\n",
    "model = MetaModel(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, preds, *_ = model(all_data)\n",
    "logs = model.recorder.compute(prefix='Valid-Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logs = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_string, value in logs.items():\n",
    "    # Precision, Recall: (2)\n",
    "    valid_logs[log_string].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(valid_logs['Valid-Time-Support_Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in valid_logs.items():\n",
    "    if k.split('_')[-1] in ['Precision', 'Recall']:\n",
    "        valid_logs[k] = (np.mean(v, axis=0), np.std(v, axis=0))\n",
    "    else:\n",
    "        valid_logs[k] = (np.mean(v), np.std(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logs['Valid-Time-Support_Precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.FloatTensor([[0.0, 0.0], [0.0, 0.0]])\n",
    "targets = torch.LongTensor([0, 0])\n",
    "loss = torch.FloatTensor([0.0, 0.0])\n",
    "lr = 0.276\n",
    "\n",
    "recorder.update('Support_Accuracy', scores, targets) #\n",
    "recorder.update('Support_Loss', loss)#\n",
    "recorder.update('Query_Accuracy', scores, targets) #\n",
    "recorder.update('Query_Loss', loss) #\n",
    "recorder.update('Finetune_Accuracy', scores, targets) #\n",
    "recorder.update('Finetune_Loss', loss) #\n",
    "recorder.update('Finetuning_LR', lr) #\n",
    "recorder.update('Inner_LR', lr) #\n",
    "recorder.update('KLD_Loss', loss) #\n",
    "recorder.update('Orthogonality_Loss', loss)  # \n",
    "recorder.update('Total_Loss', loss) #\n",
    "recorder.update('Z_Loss', loss) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.update_window_metrics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.FloatTensor([[0.4, 1.2], [3.1, 1.2]])\n",
    "targets = torch.LongTensor([1, 0])\n",
    "loss = torch.FloatTensor([1.7, 1.6])\n",
    "lr = 0.14\n",
    "\n",
    "recorder.update('Support_Accuracy', scores, targets) #\n",
    "recorder.update('Support_Loss', loss)#\n",
    "recorder.update('Query_Accuracy', scores, targets) #\n",
    "recorder.update('Query_Loss', loss) #\n",
    "recorder.update('Finetune_Accuracy', scores, targets) #\n",
    "recorder.update('Finetune_Loss', loss) #\n",
    "recorder.update('Finetuning_LR', lr) #\n",
    "recorder.update('Inner_LR', lr) #\n",
    "recorder.update('KLD_Loss', loss) #\n",
    "recorder.update('Orthogonality_Loss', loss)  # \n",
    "recorder.update('Total_Loss', loss) #\n",
    "recorder.update('Z_Loss', loss) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.update_window_metrics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = recorder.get_log_data('Train')\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.extract_query_loss_acc(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_universe(seed, stock_names):\n",
    "    stocks = {}\n",
    "    np.random.seed(seed)\n",
    "    all_idx = np.arange(len(ps))\n",
    "    train_idx = np.random.choice(all_idx, size=(int(len(ps)*0.7)), replace=False)\n",
    "    valid_test_idx = all_idx[~np.isin(all_idx, train_idx)]\n",
    "    valid_idx = np.random.choice(valid_test_idx, size=(int(len(valid_test_idx)*(0.2/0.3))), replace=False)\n",
    "    test_idx = valid_test_idx[~np.isin(valid_test_idx, valid_idx)]\n",
    "    stocks['train'] = list(stock_names[train_idx])\n",
    "    stocks['valid'] = list(stock_names[valid_idx])\n",
    "    stocks['test'] = list(stock_names[test_idx])\n",
    "    stocks['seed'] = seed\n",
    "    return stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = list((meta_train.data_dir / 'kdd17/price_long_50').glob('*.csv'))\n",
    "stock_names = np.array([p.name.rstrip('.csv') for p in ps])\n",
    "stocks = create_universe(seed=7, stock_names=stock_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with (meta_train.data_dir / 'kdd17'/ 'stock_universe.json').open('w') as file:\n",
    "    json.dump(stocks, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = list((meta_train.data_dir / 'stocknet-dataset/price/raw').glob('*.csv'))\n",
    "stock_names = np.array([p.name.rstrip('.csv') for p in ps])\n",
    "stocks = create_universe(seed=7, stock_names=stock_names)\n",
    "\n",
    "with (meta_train.data_dir / 'stocknet-dataset'/ 'stock_universe.json').open('w') as file:\n",
    "    json.dump(stocks, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "ps = list((meta_train.data_dir / 'kdd17/price_long_50').glob('*.csv'))\n",
    "with (Path('../data').resolve() / 'kdd17/stock_universe.json').open('r') as file:\n",
    "    universe_dict = json.load(file)\n",
    "\n",
    "universe_key = 'known'\n",
    "universe = universe_dict['0'][universe_key]\n",
    "iterator = [p for p in ps if p.name.strip('.csv') in universe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = iterator[29]\n",
    "stock_symbol = p.name.rstrip('.csv')\n",
    "df_single = meta_train.load_single_stock(p)\n",
    "df_single = df_single.loc[df_single[\"date\"].between(\"2014-01-01\", '2015-01-01')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = p.name.strip('.csv') # 'AMZN'\n",
    "window_size = 5\n",
    "n_support = 4\n",
    "df_stock = meta_train.data[symbol]\n",
    "labels_indices = meta_train.candidates[symbol]\n",
    "labels_candidates = labels_indices[labels_indices >= window_size]\n",
    "idx = meta_train.get_possible_idx(df_stock, labels_candidates)\n",
    "labels_candidates = labels_candidates[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.loc[:15, ['date', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_q = np.array([labels_candidates[0]])\n",
    "y_qs = y_q - window_size\n",
    "query, query_labels = meta_train.generate_data(df_stock, y_start=y_qs, y_end=y_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.loc[10:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.loc[:6, ['date', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_idx(df_stock, labels_candidates):\n",
    "    i = 0\n",
    "    while i < len(labels_candidates):\n",
    "        rise, fall = get_rise_fall(df_stock, labels_candidates, idx=i)\n",
    "        if len(rise) + len(fall) == 4:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "def get_rise_fall(df_stock, labels_candidates, idx):\n",
    "    df_check = df_stock.loc[labels_candidates[:idx], 'label'].sort_index(ascending=False)\n",
    "    rise = df_check.index[df_check == meta_train.labels_dict['rise']][:(n_support // 2)].to_numpy()\n",
    "    fall = df_check.index[df_check == meta_train.labels_dict['fall']][:(n_support // 2)].to_numpy()\n",
    "    return rise, fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unpossible candidates\n",
    "idx = get_possible_idx(df_stock, labels_candidates)\n",
    "labels_candidates = labels_candidates[idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_q = np.array(np.random.choice(labels_candidates, size=(5,), replace=False))\n",
    "y_qs = y_q - window_size\n",
    "query, query_labels = meta_train.generate_data(df_stock, y_start=y_qs, y_end=y_q)\n",
    "support = []\n",
    "support_labels = []\n",
    "for q in y_q:\n",
    "    q_idx = np.arange(len(labels_candidates))[labels_candidates == q][0]\n",
    "    rise, fall = get_rise_fall(df_stock, labels_candidates, idx=q_idx)\n",
    "    y_s = np.concatenate([fall, rise])\n",
    "    y_ss = y_s - window_size\n",
    "    data_s, label_s = meta_train.generate_data(df_stock, y_start=y_ss, y_end=y_s)\n",
    "    data_s = np.array(data_s)\n",
    "    support.append(data_s)\n",
    "    support_labels.append(label_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.expand_dims(query_labels, 1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(support).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = y_q[0]\n",
    "q_idx = np.arange(len(labels_candidates))[labels_candidates == q][0]\n",
    "rise, fall = get_rise_fall(df_stock, labels_candidates, idx=q_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = np.concatenate([fall, rise])\n",
    "y_ss = y_s - window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, support_labels = meta_train.generate_data(df_stock, y_start=y_ss, y_end=y_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'AMZN'\n",
    "window_size = 5\n",
    "n_shot = 2\n",
    "df_stock = meta_train.data[symbol]\n",
    "labels_indices = meta_train.candidates[symbol]\n",
    "y_cand = labels_indices[labels_indices >= window_size]\n",
    "n_rise = 0\n",
    "n_fall = 0\n",
    "support= []\n",
    "support_sample = []\n",
    "query = []\n",
    "support_turn = True\n",
    "query_turn = False\n",
    "query_sample = []\n",
    "for idx in y_cand:\n",
    "\n",
    "    # ex. k = 2\n",
    "    if support_turn and  n_rise < n_shot or n_fall < n_shot:\n",
    "        if n_rise < 2 and df_stock['label'][idx] == 1:\n",
    "            n_rise +=1\n",
    "            support_sample.append(idx)\n",
    "        elif n_fall < 2 and df_stock['label'][idx] == 0:\n",
    "            n_fall +=1\n",
    "            support_sample.append(idx)\n",
    "        continue\n",
    "\n",
    "    if n_rise == n_shot and n_fall == n_shot:\n",
    "        support.append(support_sample)\n",
    "        support_sample = []\n",
    "        n_rise = 0\n",
    "        n_fall = 0\n",
    "        query_turn = True\n",
    "        support_turn = False \n",
    "\n",
    "    if query_turn:\n",
    "        query_sample.append(idx)\n",
    "        query.append(query_sample)\n",
    "        query_sample = []\n",
    "        query_turn = False\n",
    "        support_turn = True\n",
    "        continue\n",
    "support_idx_set = np.array(support)\n",
    "query_idx_set = np.array(query)\n",
    "print(len(support_idx_set), len(query_idx_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_indices = self.candidates[symbol]\n",
    "labels_candidates = labels_indices[labels_indices >= window_size]\n",
    "y_s = np.array(sorted(np.random.choice(labels_candidates, size=(self.n_sample,), replace=False)))\n",
    "y_ss = y_s-window_size\n",
    "support, support_labels = self.generate_data(df_stock, y_start=y_ss, y_end=y_s)\n",
    "\n",
    "# code for jumpped tags like [1(support), 0, 0, 1(query)]\n",
    "# y_q = labels_indices[np.arange(len(labels_indices))[np.isin(labels_indices, y_s)] + self.n_lag]\n",
    "y_q = y_s + self.n_lag\n",
    "y_qs = y_s - window_size if self.keep_support_history else y_q - window_size\n",
    "query, query_labels = self.generate_data(df_stock, y_start=y_qs, y_end=y_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SMILE-3RbiCpML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee66b3967864d0acde953cfdc6a67f0a5a0d6d0589054c272a5ca1fe7c198375"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

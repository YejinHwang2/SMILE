{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:25:36.201452: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "SEED = 1000\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "NUM_CONV_LAYERS = 4\n",
    "SAVE_INTERVAL = 100\n",
    "LOG_INTERVAL = 1\n",
    "VAL_INTERVAL = 50\n",
    "NUM_TRAIN_TASKS = 20\n",
    "NUM_TEST_TASKS = 100\n",
    "NUM_ITERATIONS = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, n_support, n_query = 5, 5, 15\n",
    "num_inner_steps = 5\n",
    "_outer_lr = 0.001\n",
    "_inner_lr = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:25:38.743872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:38.748973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:38.757212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:38.758530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 14:25:38.759627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:38.760517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:38.761420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:39.047558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:39.047972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:39.048337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 14:25:39.048714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10394 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-Preprocessing train Omniglot dataset\n",
      "\t-Preprocessing test Omniglot dataset\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    train_data = DataLoader('train', num_classes, n_support, n_query)\n",
    "    val_data = DataLoader('test', num_classes, n_support, n_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_data.generate_entire_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, padding: str = 'same'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv = layers.Conv2D(\n",
    "            filters=self.filters, kernel_size=self.kernel_size, strides=2, padding=self.padding)\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet(keras.Model):\n",
    "    def __init__(self, classes=964, shape=(28,28,1)):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=shape),\n",
    "            ConvLayer(64, 3, 'same'),\n",
    "            ConvLayer(64, 3, 'same'),\n",
    "            ConvLayer(64, 3, 'same'),\n",
    "            ConvLayer(64, 3, 'same'),\n",
    "            layers.Flatten()\n",
    "        ])\n",
    "\n",
    "        self.classification = layers.Dense(classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.classification(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=_outer_lr), \n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['Accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 256)               112448    \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  247748    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 360,196\n",
      "Trainable params: 359,684\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build((1,28,28,1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:25:55.974302: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.228789329528809, 0.10129667818546295)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model.fit(dataset, epochs=1, verbose=0)\n",
    "metrics.history['loss'][-1], metrics.history['Accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_data.generate_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = model.encoder\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "opt_fn = tf.keras.optimizers.SGD(learning_rate=_inner_lr)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics_fn = tf.keras.metrics.SparseCategoricalAccuracy(name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "model = keras.Sequential([keras.Input(shape=(28, 28, 1)), feature_extractor, prediction_layer])\n",
    "model.compile(\n",
    "    optimizer=opt_fn, \n",
    "    loss=loss_fn,\n",
    "    metrics=metrics_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 256)               112448    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,733\n",
      "Trainable params: 1,285\n",
      "Non-trainable params: 112,448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_batch = val_dataset\n",
    "for task in task_batch:\n",
    "    support, query = task\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.6747 - Accuracy: 0.3600 - val_loss: 3.6073 - val_Accuracy: 0.7333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.3627 - Accuracy: 0.7200 - val_loss: 2.9573 - val_Accuracy: 0.4267\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.6089 - Accuracy: 0.4800 - val_loss: 6.2123 - val_Accuracy: 0.6933\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.5474 - Accuracy: 0.7200 - val_loss: 4.1100 - val_Accuracy: 0.6133\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7291 - Accuracy: 0.6800 - val_loss: 2.8105 - val_Accuracy: 0.6267\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    support, \n",
    "    epochs=num_inner_steps,\n",
    "    validation_data=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.6747443675994873,\n",
       "  3.3626832962036133,\n",
       "  2.6089394092559814,\n",
       "  6.547420501708984,\n",
       "  3.7291290760040283],\n",
       " 'Accuracy': [0.36000001430511475,\n",
       "  0.7200000286102295,\n",
       "  0.47999998927116394,\n",
       "  0.7200000286102295,\n",
       "  0.6800000071525574],\n",
       " 'val_loss': [3.6072559356689453,\n",
       "  2.9572553634643555,\n",
       "  6.212284564971924,\n",
       "  4.1100358963012695,\n",
       "  2.8104536533355713],\n",
       " 'val_Accuracy': [0.7333333492279053,\n",
       "  0.4266666769981384,\n",
       "  0.6933333277702332,\n",
       "  0.6133333444595337,\n",
       "  0.6266666650772095]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "logits = model.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['Accuracy']\n",
    "val_acc = history.history['val_Accuracy']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7733333110809326,\n",
       " 0.9066666960716248,\n",
       " 0.9066666960716248,\n",
       " 0.9066666960716248,\n",
       " 0.9200000166893005]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loss_batch = []\n",
    "accuracies_support_batch = []\n",
    "accuracy_query_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(support, epochs=num_inner_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = [layers.Input(shape=(28,28,1))]\n",
    "\n",
    "for i in range(NUM_CONV_LAYERS):\n",
    "    model_layers.append(\n",
    "        layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\", name=f\"Conv{i+1}\")\n",
    "    )\n",
    "    model_layers.append(\n",
    "        layers.BatchNormalization(name=f\"BN{i+1}\")\n",
    "    )\n",
    "    model_layers.append(\n",
    "        layers.ReLU(name=f\"ReLU{i+1}\")\n",
    "    )\n",
    "model_layers.append(layers.Flatten())\n",
    "model_layers.append(layers.Dense(num_classes, activation='softmax', name='Classification'))\n",
    "model = keras.Sequential(model_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = val_data.generate_task(NUM_TEST_TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = train_data.generate_task(NUM_TRAIN_TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_optimizer = keras.optimizers.Adam(learning_rate=_outer_lr)\n",
    "\n",
    "theta = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), model.trainable_weights)\n",
    "tf.nest.map_structure(lambda x, y: x.assign(y), theta, model.trainable_weights)\n",
    "\n",
    "task_batch = train_task\n",
    "for task in task_batch:\n",
    "    support, query = task\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(12, 10))\n",
    "for i in range(5):\n",
    "    imgs, label = support.take(1).get_single_element()\n",
    "    axes[i].imshow(imgs[0].numpy().squeeze())\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_run(theta, support_data):\n",
    "    accuracies = []\n",
    "    \n",
    "    phi = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), model.trainable_weights)\n",
    "    tf.nest.map_structure(lambda x, y: x.assign(y), phi, theta)\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    opt_fn = tf.keras.optimizers.SGD(learning_rate=_inner_lr)\n",
    "    metrics_fn = tf.keras.metrics.SparseCategoricalAccuracy(name='Inner Accuracy')\n",
    "    for _ in range(num_inner_steps):\n",
    "        for imgs, label in support_data:\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(phi)\n",
    "                logits = model(imgs, training=True)\n",
    "                loss = loss_fn(label, logits)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            opt_fn.apply_gradients(zip(grads, phi))\n",
    "            opt_fn.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            metrics_fn.update_state(label, logits)\n",
    "            accuracies.append(metrics_fn.result().numpy())\n",
    "    return phi, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi, accuracies = inner_run(theta, support_data=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics_fn = tf.keras.metrics.SparseCategoricalAccuracy(name='Outer Accuracy')\n",
    "all_grads = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), model.trainable_weights)\n",
    "\n",
    "outer_loss_batch = []\n",
    "accuracies_support_batch = []\n",
    "accuracy_query_batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta_{t+1} \\coloneqq \\theta_t - \\beta \\sum_{\\tau_i \\sim p(\\tau)} \\triangledown_\\theta  L(\\phi_i^L, D_i^{query})$$\n",
    "\n",
    "\n",
    "$$\\triangledown_\\theta L(\\phi_i^L, D_i^{query}) = \\triangledown_{\\phi_i^L} L(\\phi_i^L, D_i^{query}) \\cdot \\prod_{k=1}^L (I - \\alpha \\triangledown_{\\phi_i^{k-1}} (\\triangledown_\\theta L(\\phi_i^{k-1}, D_i^{query}) ) ) $$\n",
    "\n",
    "FO-MAML\n",
    "\n",
    "$$\\triangledown_\\theta L(\\phi_i^L, D_i^{query}) \\approx \\triangledown_{\\phi_i^L} L(\\phi_i^L, D_i^{query}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch \n",
    "tf.nest.map_structure(lambda x, y: x.assign(y), model.trainable_weights, phi)\n",
    "single_task_grads = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), model.trainable_weights)\n",
    "B = len(query)\n",
    "query_loss = 0\n",
    "metrics_fn.reset_states()\n",
    "for imgs, label in (query):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        logits = model(imgs, training=train)\n",
    "        loss = loss_fn(label, logits)\n",
    "\n",
    "    query_loss += loss\n",
    "    metrics_fn.update_state(label, logits)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    single_task_grads = tf.nest.map_structure(lambda x, y: x + y, single_task_grads, grads)\n",
    "\n",
    "single_task_grads = [x/B for x in single_task_grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape.gradient(loss, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape.gradient(loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the batch loop\n",
    "metrics_fn.reset_states()\n",
    "task_loss = 0\n",
    "num_batch_data = 0\n",
    "task_grads = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), model.trainable_weights)\n",
    "\n",
    "for imgs, label in query:\n",
    "    batch_size = len(label)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(imgs, training=train)\n",
    "        loss = loss_fn(label, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    task_grads = tf.nest.map_structure(lambda x, y: x + y, task_grads, grads)\n",
    "    task_loss += loss\n",
    "    num_batch_data += batch_size\n",
    "    metrics_fn.update_state(label, logits)\n",
    "\n",
    "task_grads = [x / num_batch_data for x in task_grads]\n",
    "accuracies_support_batch.append(accuracies)\n",
    "accuracy_query_batch.append(metrics_fn.result().numpy())\n",
    "outer_loss_batch.append(task_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "all_grads = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), self.model.trainable_weights)\n",
    "\n",
    "for task in task_batch:\n",
    "    support, query = task\n",
    "    # support\n",
    "    phi, accuracies = self._inner_loop(theta, support_data=support)\n",
    "    # query\n",
    "    metrics_fn.reset_states()\n",
    "    query_loss = 0\n",
    "    num_batch_data = 0\n",
    "    task_grads = tf.nest.map_structure(lambda x: tf.Variable(tf.zeros_like(x)), self.model.trainable_weights)\n",
    "\n",
    "    for imgs, label in query:\n",
    "        batch_size = len(label)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(imgs, training=train)\n",
    "            loss = loss_fn(label, logits)\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        task_grads = tf.nest.map_structure(lambda x, y: x + y, task_grads, grads)\n",
    "        query_loss += loss\n",
    "        num_batch_data += batch_size\n",
    "        metrics_fn.update_state(label, logits)\n",
    "\n",
    "    task_grads = [x / num_batch_data for x in task_grads]\n",
    "    all_grads = tf.nest.map_structure(lambda x, y: x + y, task_grads, all_grads)\n",
    "    accuracies_support_batch.append(accuracies)\n",
    "    accuracy_query_batch.append(metrics_fn.result().numpy())\n",
    "    outer_loss_batch.append(query_loss)\n",
    "\n",
    "self._optimizer.apply_gradients(zip(all_grads, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grads = tf.nest.map_structure(lambda x, y: x + y, task_grads, all_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_optimizer.apply_gradients(zip(all_grads, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_data.generate_entire_dataset()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72e99d6c0fcb83ce0177dd31749d041162c640b5d8b93123e680ce71660def03"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('metalearning-rjmJ2xbB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
